{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hamiltonian Monte Carlo for linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Likelihood and priors</b>\n",
    "\n",
    "In linear regression, we are given a dataset $\\mathcal{D}=\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^N$ and the objective is to fit a model of form $p(y|\\mathbf{w}, \\mathbf{x}) = \\mathcal{N}(y | \\mathbf{w}^T\\mathbf{x} + b, \\sigma^2)$ to this data.  \n",
    "\n",
    "To simplify the notation a bit, let us define the design matrix\n",
    "$$\n",
    "X = \\begin{pmatrix}\n",
    "x_1^1 & x_1^2 & ... & x_1^d & 1\\\\\n",
    "x_2^1 & x_2^2 & ... & x_2^d & 1\\\\\n",
    ". & .\\\\\n",
    ". & .\\\\\n",
    ". & .\\\\\n",
    "x_N^1 & x_N^2 & ... & x_N^d & 1\\\\\n",
    "\\end{pmatrix}$$  \n",
    "\n",
    "Let us also collect all parameters for the mean and the outputs into vectors $\\boldsymbol{\\theta} = [w_1, w_2, ..., w_d, b]^T$ and $\\mathbf{y} = [y_1, y_2, ..., y_N]^T$  \n",
    "\n",
    "Then\n",
    "  \n",
    "$$\\mathbf{X}\\boldsymbol{\\theta} = \\begin{pmatrix}\n",
    "\\mathbf{w}^T\\mathbf{x}_1 + b\\\\\n",
    "\\mathbf{w}^T\\mathbf{x}_2 + b\\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    "\\mathbf{w}^T\\mathbf{x}_N + b\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "We next define noninformative priors for all model parameters. For the parameters $\\boldsymbol{\\theta}$, the support is $\\mathbb{R}^{d+1}$. A natural choice is therefore $p(\\boldsymbol{\\theta}) = \\mathcal{N(\\mathbf{0}, \\sigma_0^2\\mathbf{I})}$, where the variance $\\sigma_0^2$ is large.\n",
    "\n",
    "We place the prior of the noise over the precision $\\tau = \\sigma^{-2}$. The support is $[0, \\infty)$, so a natural choice is $p(\\tau) = \\Gamma(\\alpha_0, \\beta_0)$\n",
    "\n",
    "<b>Posterior</b>\n",
    "\n",
    "The posterior distribution is $$p(\\boldsymbol{\\theta}, \\tau | \\mathcal{D}) = \\frac{p(\\boldsymbol{\\theta}, \\tau)p(\\mathbf{y} | \\mathbf{X}, \\boldsymbol{\\theta}, \\tau)}{Z} \\propto \\exp(-\\frac{1}{2\\sigma_0^2}\\boldsymbol{\\theta}^T \\boldsymbol{\\theta}) \\tau^{\\alpha_0 - 1} \\exp(-\\beta_0\\tau) \\prod_{i=1}^N \\sqrt{\\frac{\\tau}{2\\pi}}\\exp(-\\frac{\\tau}{2}(y_i - \\mathbf{w}^T\\mathbf{x}_i + b)^2)$$\n",
    "\n",
    "In the likelihood, we can take the exponentiation outside the product and then combine terms which gives\n",
    "\n",
    "$$p(\\boldsymbol{\\theta}, \\tau | \\mathcal{D}) \\propto \\tau^{\\alpha_0 + N/2 - 1} \\exp(\\sum_{i=1}^N (-\\frac{\\tau}{2}(y_i - \\mathbf{w}^T\\mathbf{x}_i + b)^2) - \\frac{1}{2\\sigma_0^2}\\boldsymbol{\\theta}^T \\boldsymbol{\\theta} - \\beta_0\\tau) = \\tau^{\\alpha_0 + N/2 - 1} \\exp(-\\frac{\\tau}{2}(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\theta})^T(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\theta}) - \\frac{1}{2\\sigma_0^2}\\boldsymbol{\\theta}^T \\boldsymbol{\\theta} - \\beta_0\\tau)$$\n",
    "\n",
    "<b>Conditional distributions</b>\n",
    "\n",
    "The posterior above does not resemble any known distribution, so we will use MCMC to sample from the posterior. We will use a Gibbs sampling scheme that alternates between sampling from the conditional distributions $p(\\boldsymbol{\\theta}|\\tau, \\mathcal{D})$ and $p(\\tau|\\boldsymbol{\\theta}, \\mathcal{D})$. For sampling the weights and the bias from $p(\\boldsymbol{\\theta}|\\tau, \\mathcal{D})$, we will use HMC. However, HMC is not optimal for sampling the precision from $p(\\tau|\\boldsymbol{\\theta}, \\mathcal{D})$, since the support is constrained to be positive. The prior we have chosen for $\\tau$ is conjugate to the gaussian likelihood, so we conveniently get\n",
    "\n",
    "$$p(\\tau|\\boldsymbol{\\theta}, \\mathcal{D}) \\propto \\tau^{\\alpha_0 + N/2 - 1}\\exp(-\\tau(\\frac{1}{2}(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\theta})^T(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\theta}) + \\beta_0)) $$\n",
    "\n",
    "We can now recognize that this has the form of a gamma distribution. The parameters are given by\n",
    "\n",
    "$$\\alpha - 1 = \\alpha_0 + N/2 - 1 \\Rightarrow \\alpha = \\alpha_0 + N/2$$\n",
    "$$\\beta = \\beta_0 + \\frac{1}{2}(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\theta})^T(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\theta})$$\n",
    "\n",
    "We can conclude that $p(\\tau|\\boldsymbol{\\theta}, \\mathcal{D}) = \\Gamma(\\alpha, \\beta)$. <i>Note:</i> The reason why we obtain the exact distribution despite dropping constants is that we only need to match the factors that depend on $\\tau$, since then the normalization constants will also match as both distributions must integrate to 1.\n",
    "\n",
    "Next, we are going to derive the unnormalized distribution for $\\boldsymbol{\\theta}$, which we will then sample from with HMC.\n",
    "\n",
    "$$p(\\boldsymbol{\\theta} | \\tau, \\mathcal{D}) \\propto \\exp(-\\frac{1}{2}(\\tau(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\theta})^T(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\theta}) + \\frac{1}{\\sigma_0^2}\\boldsymbol{\\theta}^T \\boldsymbol{\\theta}))$$\n",
    "\n",
    "<b>Energy functions and gradients</b>\n",
    "\n",
    "In HMC, the energy (or Hamiltonian) $H(\\mathbf{s})$of a state $\\mathbf{s}$ is related to its probability by the canonical ensemble\n",
    "\n",
    "$$p(\\mathbf{s}) = \\frac{1}{Z}\\exp(-H(\\mathbf{s})/T)$$\n",
    "\n",
    "where $T$ is a temperature parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b>Hamiltonian mechanics and leapfrog integration</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
