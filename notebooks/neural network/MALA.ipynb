{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c593e209",
   "metadata": {},
   "source": [
    "# Metropolis-adjusted Langevin algorithm Implementations for nonlinear regression with neural network\n",
    "\n",
    "The Metropolis-adjusted Langevin algorithm (MALA) is a Markov Chain Monte Carlo (MCMC) method for obtaining random samples from a probability distribution for which direct sampling is difficult. MALA uses a combination of two mechanisms to generate the states of a random walk\n",
    "- New states are proposed using Langevin dynamics: use evaluations of the gradient of the target probability density function\n",
    "- Proposals are accepted or rejected using Metropolis-Hasting algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ec6d81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "868eefff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select device which you are going to use for training\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373632a8",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "Helper functions imported from helper files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b811c923",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simulator:\n",
    "    \n",
    "    def __init__(self, w, b, sigma, N, design_range=(-10,10)):\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "        self.theta = np.expand_dims(np.concatenate([w, [b]], axis=0), axis=1)\n",
    "        self.sigma = sigma\n",
    "        self.N = N\n",
    "        self.design_range = design_range\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.y_mean = None\n",
    "        \n",
    "    def run(self):\n",
    "        designs = np.random.uniform(self.design_range[0], self.design_range[1], size=(self.N, self.w.size))\n",
    "        self.X = np.concatenate([designs, np.ones((self.N, 1))], axis=1)\n",
    "        self.y_mean = (self.X @ self.theta).squeeze()\n",
    "        self.y = np.random.multivariate_normal(mean=self.y_mean, cov=np.diag([self.sigma**2] * self.N))\n",
    "    \n",
    "    def plot(self):\n",
    "        x = self.X[:, 0]\n",
    "        plt.scatter(x, self.y, label=\"data\")\n",
    "        x_dense = np.linspace(self.design_range[0], self.design_range[1], 100)\n",
    "        y_dense = x_dense * self.w[0] + self.b\n",
    "        plt.plot(x_dense, y_dense, label=\"y mean\")\n",
    "        plt.xlabel(\"x\")\n",
    "        plt.ylabel(\"y\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Simulated data, N=\"+str(self.N))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "06ceba8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for collecting nn gradient into a vector\n",
    "def collect_grads(model):\n",
    "    return torch.cat([p.grad.data.view(1, -1) for p in model.parameters()], dim=-1)\n",
    "\n",
    "# Helper function for computing sizes of all nn parameters\n",
    "def get_param_sizes(model):\n",
    "    return [p.reshape(-1).size()[0] for p in model.parameters()]\n",
    "\n",
    "# Helper function for writing the updated weights\n",
    "def update_params(new_params, model, param_sizes):\n",
    "    start_index = 0\n",
    "    for i, p in enumerate(model.parameters()):\n",
    "        end_index = start_index + param_sizes[i]\n",
    "        source_tensor = new_params[:, start_index:end_index].reshape(p.shape)\n",
    "        p.data = source_tensor\n",
    "        start_index = end_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5ade517e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# True weight(s)\n",
    "w = np.array([1.5, -1.0, 0.7])\n",
    "\n",
    "# Input dimensionality\n",
    "d = w.size\n",
    "\n",
    "# True intercept\n",
    "b = 0.5\n",
    "\n",
    "# True standard deviation\n",
    "sigma = 0.5\n",
    "\n",
    "# Number of data points\n",
    "N = 100\n",
    "\n",
    "# Defines range of inputs x\n",
    "design_range = (-1.0, 1.0)\n",
    "\n",
    "# Simulate\n",
    "simulator = Simulator(w, b, sigma, N, design_range)\n",
    "simulator.run()\n",
    "\n",
    "X = simulator.X\n",
    "y = simulator.y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c92a6b0",
   "metadata": {},
   "source": [
    "### Step 1: Implement function that constructs MLP neural network\n",
    "It would be good to not hardcode the amount of hidden layers, layer dimensions or activation functions but instead make it so that these can be given as arguments to the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57f6fea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# True weight(s)\n",
    "w = np.array([1.5, -1.0, 0.7])\n",
    "\n",
    "# Input dimensionality\n",
    "d = w.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c3fec6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.layer_1 = nn.Linear(d + 1, 16)\n",
    "        self.layer_2 = nn.Linear(16, 32)\n",
    "        self.output_layer = nn.Linear(16, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.layer_1(x))\n",
    "        x = torch.sigmoid(self.layer_2(x))\n",
    "        return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0b8fd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with one hidden layer: 2 inputs and 1 output\n",
    "# Replace log into L2_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4557641",
   "metadata": {},
   "source": [
    "### Step 2: Evaluate and Sample \n",
    "Evaluate log \n",
    "$$\n",
    "log_p(\\theta | \\alpha^2, D) \\propto \\frac{-1}{2\\alpha^2} (y- f_\\theta(x))^T (y - f_\\theta(x)) - \\frac{-1}{2\\alpha_0^2} \\theta^T\\theta $$\n",
    "\n",
    "where:\n",
    "- $log_p(\\theta | \\alpha^2, D)$ is L2 loss\n",
    "- $f_\\theta(x)$ is the neural network\n",
    "- $\\theta$ is a vector that contains all parameters of the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b3e0edda",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP()\n",
    "\n",
    "# Get all the nn parameters and store in theta\n",
    "theta = torch.cat([x.reshape(-1) for x in mlp.parameters()])\n",
    "\n",
    "# L2 Loss calculation\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32, requires_grad=False)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32, requires_grad=False).view(1, -1)\n",
    "#l2_loss = ((y_tensor - mlp(X_tensor)) ** 2).sum()\n",
    "l2_loss = 203.51\n",
    "\n",
    "# Define alpha 0\n",
    "alpha_0 = 0.0001\n",
    "\n",
    "# Define f0(x)\n",
    "f0_x = mlp(X_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8731434a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WIP: Implement MALA and get value y\n",
    "def sample_MALA():\n",
    "    y = 0\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b13a887",
   "metadata": {},
   "source": [
    "#### Weight-decay regulization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "47bb6221",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(model, x1, x2, y):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model.forward(x1,x2)\n",
    "        loss = F.mse_loss(outputs, y)\n",
    "        return loss.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a7953eb4",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (100x32 and 16x1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [103]\u001b[0m, in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[1;32m     18\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 19\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(outputs, y_train)\n\u001b[1;32m     21\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36mMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_1(x))\n\u001b[1;32m     11\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_2(x))\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/software/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/software/lib/python3.9/site-packages/torch/nn/modules/linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/software/lib/python3.9/site-packages/torch/nn/functional.py:1848\u001b[0m, in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[1;32m   1847\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(linear, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, weight, bias\u001b[38;5;241m=\u001b[39mbias)\n\u001b[0;32m-> 1848\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (100x32 and 16x1)"
     ]
    }
   ],
   "source": [
    "# Create an Adam optimizer with learning rate 0.01 and weight decay parameter 0.001\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=0.01, weight_decay = 0.001)\n",
    "\n",
    "# Train network with \n",
    "n_epochs = 4000\n",
    "mlp.zero_grad()\n",
    "train_errors = []\n",
    "val_errors = []\n",
    "\n",
    "# Convert x and Y from numpy to tensor\n",
    "x_train, y_train = torch.from_numpy(X), torch.from_numpy(y)\n",
    "x_train, y_train = x_train.type(torch.FloatTensor),y_train.type(torch.FloatTensor)\n",
    "\n",
    "x = x_train.to(device)\n",
    "y = y_train.to(device)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = mlp.forward(x_train)\n",
    "    loss = F.mse_loss(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        train_errors.append(compute_loss(mlp, x_train, y_train))\n",
    "        val_errors.append(compute_loss(mlp, x_test, y_test))\n",
    "        print_progress(epoch, train_errors[-1], val_errors[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c82898d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 200\n",
    "N = 1000\n",
    "\n",
    "def evaluate(f0_x, theta, alpha_0, y):\n",
    "    for i in range(T):\n",
    "        alpha = alpha_0 + N/2\n",
    "        \n",
    "        # Calculate right handside\n",
    "        b = (y - f0_x)\n",
    "        A = -1/(2*alpha**2)* b.T @ b - (-1/(2*alpha_0**2) * theta.T @ theta)\n",
    "        \n",
    "    return A\n",
    "        \n",
    "# evaluate(f0_x, theta, alpha_0, y = sample_MALA())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f72ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot right handside and left handside value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f32adc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c48d44e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3642ed9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
