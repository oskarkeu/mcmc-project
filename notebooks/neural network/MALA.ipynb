{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c593e209",
   "metadata": {},
   "source": [
    "# Metropolis-adjusted Langevin algorithm Implementations for nonlinear regression with neural network\n",
    "\n",
    "The Metropolis-adjusted Langevin algorithm (MALA) is a Markov Chain Monte Carlo (MCMC) method for obtaining random samples from a probability distribution for which direct sampling is difficult. MALA uses a combination of two mechanisms to generate the states of a random walk\n",
    "- New states are proposed using Langevin dynamics: use evaluations of the gradient of the target probability density function\n",
    "- Proposals are accepted or rejected using Metropolis-Hasting algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "2ec6d81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "4ac84fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select device which you are going to use for training\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8d69ce",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "Helper functions imported from helper files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "71ab391a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simulator:\n",
    "    \n",
    "    def __init__(self, w, b, sigma, N, design_range=(-10,10)):\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "        self.theta = np.expand_dims(np.concatenate([w, [b]], axis=0), axis=1)\n",
    "        self.sigma = sigma\n",
    "        self.N = N\n",
    "        self.design_range = design_range\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.y_mean = None\n",
    "        \n",
    "    def run(self):\n",
    "        designs = np.random.uniform(self.design_range[0], self.design_range[1], size=(self.N, self.w.size))\n",
    "        self.X = np.concatenate([designs, np.ones((self.N, 1))], axis=1)\n",
    "        self.y_mean = (self.X @ self.theta).squeeze()\n",
    "        self.y = np.random.multivariate_normal(mean=self.y_mean, cov=np.diag([self.sigma**2] * self.N))\n",
    "    \n",
    "    def plot(self):\n",
    "        x = self.X[:, 0]\n",
    "        plt.scatter(x, self.y, label=\"data\")\n",
    "        x_dense = np.linspace(self.design_range[0], self.design_range[1], 100)\n",
    "        y_dense = x_dense * self.w[0] + self.b\n",
    "        plt.plot(x_dense, y_dense, label=\"y mean\")\n",
    "        plt.xlabel(\"x\")\n",
    "        plt.ylabel(\"y\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Simulated data, N=\"+str(self.N))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "93069690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for collecting nn gradient into a vector\n",
    "def collect_grads(model):\n",
    "    return torch.cat([p.grad.data.view(1, -1) for p in model.parameters()], dim=-1)\n",
    "\n",
    "# Helper function for computing sizes of all nn parameters\n",
    "def get_param_sizes(model):\n",
    "    return [p.reshape(-1).size()[0] for p in model.parameters()]\n",
    "\n",
    "# Helper function for writing the updated weights\n",
    "def update_params(new_params, model, param_sizes):\n",
    "    start_index = 0\n",
    "    for i, p in enumerate(model.parameters()):\n",
    "        end_index = start_index + param_sizes[i]\n",
    "        source_tensor = new_params[:, start_index:end_index].reshape(p.shape)\n",
    "        p.data = source_tensor\n",
    "        start_index = end_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "68cb1cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 4), (100,))"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# True weight(s)\n",
    "w = np.array([1.5, -1.0, 0.7])\n",
    "\n",
    "# Input dimensionality\n",
    "d = w.size\n",
    "\n",
    "# True intercept\n",
    "b = 0.5\n",
    "\n",
    "# True standard deviation\n",
    "sigma = 0.5\n",
    "\n",
    "# Number of data points\n",
    "N = 100\n",
    "\n",
    "# Defines range of inputs x\n",
    "design_range = (-1.0, 1.0)\n",
    "\n",
    "# Simulate\n",
    "simulator = Simulator(w, b, sigma, N, design_range)\n",
    "simulator.run()\n",
    "\n",
    "X = simulator.X\n",
    "y = simulator.y\n",
    "\n",
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c92a6b0",
   "metadata": {},
   "source": [
    "### Step 1: Implement function that constructs MLP neural network\n",
    "It would be good to not hardcode the amount of hidden layers, layer dimensions or activation functions but instead make it so that these can be given as arguments to the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "2ae6847b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# True weight(s)\n",
    "w = np.array([1.5, -1.0, 0.7])\n",
    "\n",
    "# Input dimensionality\n",
    "d = w.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "c540752c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.layer_1 = nn.Linear(d + 1, 16)\n",
    "        self.layer_2 = nn.Linear(16, 16)\n",
    "        self.output_layer = nn.Linear(16, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.layer_1(x))\n",
    "        x = torch.sigmoid(self.layer_2(x))\n",
    "        return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "c0b8fd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with one hidden layer: 2 inputs and 1 output\n",
    "# Replace log into L2_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91020a2",
   "metadata": {},
   "source": [
    "### Step 2: Evaluate and Sample \n",
    "Evaluate log \n",
    "$$\n",
    "log_p(\\theta | \\alpha^2, D) \\propto \\frac{-1}{2\\alpha^2} (y- f_\\theta(x))^T (y - f_\\theta(x)) - \\frac{-1}{2\\alpha_0^2} \\theta^T\\theta $$\n",
    "\n",
    "where:\n",
    "- $log_p(\\theta | \\alpha^2, D)$ is L2 loss\n",
    "- $f_\\theta(x)$ is the neural network\n",
    "- $\\theta$ is a vector that contains all parameters of the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "0d20bf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP()\n",
    "\n",
    "# Get all the nn parameters and store in theta\n",
    "theta = torch.cat([x.reshape(-1) for x in mlp.parameters()])\n",
    "\n",
    "# L2 Loss calculation\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32, requires_grad=False)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32, requires_grad=False).view(1, -1)\n",
    "l2_loss = ((y_tensor - mlp(X_tensor)) ** 2).sum()\n",
    "\n",
    "# Define alpha 0\n",
    "alpha_0 = 0.0001\n",
    "\n",
    "# Define f0(x)\n",
    "f0_x = mlp(X_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961c08ca",
   "metadata": {},
   "source": [
    "#### Implement MALA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "2fb349e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WIP: Implement MALA and get value y\n",
    "def sample_MALA():\n",
    "    y = 0\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff7c361",
   "metadata": {},
   "source": [
    "#### Weight-decay regulization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "23ed4277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the loss:\n",
    "def compute_loss(mlp, x, y):\n",
    "    mlp.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = mlp.forward(x)\n",
    "        loss = F.mse_loss(outputs, y)\n",
    "        return loss.cpu().numpy()\n",
    "\n",
    "# Print the progress during training\n",
    "def print_progress(epoch, train_error, val_error):\n",
    "    print('Epoch {}: Train error: {:.4f}, Test error: {:.4f}'.format(\n",
    "        epoch, train_error, val_error))\n",
    "    \n",
    "# This visualizes the function implemented by an MLP\n",
    "def plot_fit(mlp, x_train, y_train):\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.plot(x_train, y_train, '.')\n",
    "    x_np = np.linspace(-0.5, 0.5, 100).reshape((-1, 1))\n",
    "    x = torch.tensor(x_np, device=device, dtype=torch.float)\n",
    "    pred = mlp.forward(x).cpu().data.numpy()\n",
    "    ax.plot(x_np, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "5fc84cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert x and Y from numpy to float tensor\n",
    "x_tensor, y_tensor = torch.tensor(X).float(), torch.tensor(y).float()\n",
    "    \n",
    "# Split data into training and test sets\n",
    "# TODO: add correct train set and test set inside function\n",
    "torch.manual_seed(2)\n",
    "rp = torch.randperm(x.size(0))\n",
    "\n",
    "n_train_x = int(x_tensor.size(0) * 0.7)\n",
    "n_train_y = int(y_tensor.size(0) * 0.7)\n",
    "\n",
    "x_test, y_test = x_tensor[rp[n_train_x:]], y_tensor[rp[n_train_y:]]\n",
    "x_train, y_train = x_tensor[rp[:n_train_x]], y_tensor[rp[:n_train_y]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "9351ccb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_177/512807421.py:16: UserWarning: Using a target size (torch.Size([56])) that is different to the input size (torch.Size([70, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.mse_loss(outputs, y_train)\n",
      "/tmp/ipykernel_177/2572793256.py:6: UserWarning: Using a target size (torch.Size([56])) that is different to the input size (torch.Size([70, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.mse_loss(outputs, y)\n",
      "/tmp/ipykernel_177/2572793256.py:6: UserWarning: Using a target size (torch.Size([24])) that is different to the input size (torch.Size([10, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.mse_loss(outputs, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: Train error: 1.6426, Test error: 1.5798\n",
      "Epoch 199: Train error: 1.6426, Test error: 1.5798\n",
      "Epoch 299: Train error: 1.6426, Test error: 1.5798\n",
      "Epoch 399: Train error: 1.6426, Test error: 1.5798\n",
      "Epoch 499: Train error: 1.6426, Test error: 1.5798\n",
      "Epoch 599: Train error: 1.6426, Test error: 1.5798\n",
      "Epoch 699: Train error: 1.6426, Test error: 1.5798\n",
      "Epoch 799: Train error: 1.6426, Test error: 1.5798\n",
      "Epoch 899: Train error: 1.6426, Test error: 1.5798\n",
      "Epoch 999: Train error: 1.6426, Test error: 1.5798\n",
      "Epoch 1099: Train error: 1.6426, Test error: 1.5798\n",
      "Epoch 1199: Train error: 1.6426, Test error: 1.5798\n",
      "Epoch 1299: Train error: 1.6426, Test error: 1.5798\n",
      "Epoch 1399: Train error: 1.6426, Test error: 1.5798\n",
      "Epoch 1499: Train error: 1.6427, Test error: 1.5784\n",
      "Epoch 1599: Train error: 1.6426, Test error: 1.5798\n",
      "Epoch 1699: Train error: 1.6426, Test error: 1.5798\n",
      "Epoch 1799: Train error: 1.6426, Test error: 1.5799\n",
      "Epoch 1899: Train error: 1.6426, Test error: 1.5798\n",
      "Epoch 1999: Train error: 1.6427, Test error: 1.5821\n",
      "Epoch 2099: Train error: 1.6426, Test error: 1.5798\n",
      "Epoch 2199: Train error: 1.6426, Test error: 1.5798\n",
      "Epoch 2299: Train error: 1.6428, Test error: 1.5775\n",
      "Epoch 2399: Train error: 1.6426, Test error: 1.5799\n",
      "Epoch 2499: Train error: 1.6426, Test error: 1.5798\n",
      "Epoch 2599: Train error: 1.6426, Test error: 1.5796\n",
      "Epoch 2699: Train error: 1.6426, Test error: 1.5798\n",
      "Epoch 2799: Train error: 1.6426, Test error: 1.5798\n",
      "Epoch 2899: Train error: 1.6426, Test error: 1.5798\n",
      "Epoch 2999: Train error: 1.6426, Test error: 1.5798\n",
      "Epoch 3099: Train error: 1.6426, Test error: 1.5798\n",
      "Epoch 3199: Train error: 1.6426, Test error: 1.5800\n",
      "Epoch 3299: Train error: 1.6426, Test error: 1.5798\n",
      "Epoch 3399: Train error: 1.6426, Test error: 1.5798\n",
      "Epoch 3499: Train error: 1.6426, Test error: 1.5798\n",
      "Epoch 3599: Train error: 1.6426, Test error: 1.5798\n",
      "Epoch 3699: Train error: 1.6427, Test error: 1.5788\n",
      "Epoch 3799: Train error: 1.6426, Test error: 1.5798\n",
      "Epoch 3899: Train error: 1.6426, Test error: 1.5796\n",
      "Epoch 3999: Train error: 1.6426, Test error: 1.5798\n"
     ]
    }
   ],
   "source": [
    "# Create an Adam optimizer with learning rate 0.01 and weight decay parameter 0.001\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=0.01, weight_decay = 0.001)\n",
    "\n",
    "# Not sure if my understand is correct, need to check this out.\n",
    "# Train network with \n",
    "n_epochs = 4000\n",
    "mlp.zero_grad()\n",
    "train_errors = []\n",
    "val_errors = []\n",
    "\n",
    "x = x_train.to(device)\n",
    "y = y_train.to(device)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = mlp.forward(x_train)\n",
    "    loss = F.mse_loss(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        train_errors.append(compute_loss(mlp, x_train, y_train))\n",
    "        val_errors.append(compute_loss(mlp, x_test, y_test))\n",
    "        print_progress(epoch, train_errors[-1], val_errors[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "a41c02d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0.001\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(optimizer)\n",
    "#plot_fit(mlp, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "c82898d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 200\n",
    "N = 1000\n",
    "\n",
    "def evaluate(f0_x, theta, alpha_0, y):\n",
    "    for i in range(T):\n",
    "        alpha = alpha_0 + N/2\n",
    "        \n",
    "        # Calculate right handside\n",
    "        b = (y - f0_x)\n",
    "        A = -1/(2*alpha**2)* b.T @ b - (-1/(2*alpha_0**2) * theta.T @ theta)\n",
    "        \n",
    "    return A\n",
    "        \n",
    "# evaluate(f0_x, theta, alpha_0, y = sample_MALA())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448ed694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot right handside and left handside value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e35c277",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "42601947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f57c78612f0>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e78a83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
